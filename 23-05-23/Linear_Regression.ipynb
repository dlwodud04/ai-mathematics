{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Tp0j_HwcZNRLhuvkMB4BpLHGEVn8muZQ",
      "authorship_tag": "ABX9TyMNQEaYt5b9vKb8KLsfT6ZL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlwodud04/ai-mathematics/blob/main/23-05-23/Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**선형회귀 모델**\n",
        "- 트레이닝 데이터를 이옹하여 데이터의 특성과 상관관계 등을 학습하고, 학습 결과를 바탕으로 트레이닝 데이터에 없는 미지의 데이터에 대한 결과를 연속적이나 (숫자)값으로 예측하는 것입니다."
      ],
      "metadata": {
        "id": "tWvZEd4MGinT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**손실함수**\n",
        "- 머신러닝에서 손실함수 또는 비용함수라고 부르는 이 함수는 직선 \n",
        "y = Wx + b함수의 계산 값 y와 정답 t의 차이를 모두 더해서 수식으로 나타낸 것입니다."
      ],
      "metadata": {
        "id": "5fORWp75I9uI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**경사하강법**\n",
        "- 머신러닝에는 손실함수의 최솟값을 찾기 위한 다양한 방법이 있지만, 경사하강법이 가장 일반적이며 많이 쓰이는 방법입니다.\n"
      ],
      "metadata": {
        "id": "yZ0smqvaO6Qk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6mWDnjGCCqR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71963418-4ace-4fdb-f9ab-a879614349b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20.16386727097963 [[0.91851296]] [0.02253753]\n",
            "0 11.896023107802018 [[1.21508786]] [0.08918151]\n",
            "400 0.005680055607345394 [[2.04894102]] [0.82335063]\n",
            "800 0.0003624211280863195 [[2.01236242]] [0.95537867]\n",
            "1200 2.3124610595941837e-05 [[2.00312273]] [0.98872873]\n",
            "1600 1.4754868681000294e-06 [[2.0007888]] [0.9971529]\n",
            "2000 9.41447852250811e-08 [[2.00019925]] [0.99928083]\n",
            "2400 6.006993878830571e-09 [[2.00005033]] [0.99981834]\n",
            "2800 3.8328172266063725e-10 [[2.00001271]] [0.99995411]\n",
            "3200 2.4455639859585344e-11 [[2.00000321]] [0.99998841]\n",
            "3600 1.560414404551732e-12 [[2.00000081]] [0.99999707]\n",
            "4000 9.956366409788292e-14 [[2.0000002]] [0.99999926]\n",
            "4400 6.352750337430288e-15 [[2.00000005]] [0.99999981]\n",
            "4800 4.0534303591340675e-16 [[2.00000001]] [0.99999995]\n",
            "5200 2.5863281154551174e-17 [[2.]] [0.99999999]\n",
            "5600 1.6502305681798824e-18 [[2.]] [1.]\n",
            "6000 1.0529434562171054e-19 [[2.]] [1.]\n",
            "6400 6.718493716696574e-21 [[2.]] [1.]\n",
            "6800 4.2867345619697105e-22 [[2.]] [1.]\n",
            "7200 2.735678379654171e-23 [[2.]] [1.]\n",
            "7600 1.74815016030523e-24 [[2.]] [1.]\n",
            "8000 1.118349172670103e-25 [[2.]] [1.]\n"
          ]
        }
      ],
      "source": [
        "# simple regression\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Step 1 . 학습데이터 준비\n",
        "x_data = np.array([1,2,3,4,5]).reshape(5,1) \n",
        "t_data = np.array([3,5,7,9,11]).reshape(5,1)\n",
        "\n",
        "# Step 2. 임의의 직선 y = W+b 정의(임의의 값으로 가중치 w, 바이어스 b 초기화)\n",
        "W = np.random.rand(1,1)\n",
        "b = np.random.rand(1)\n",
        "\n",
        "# Step 3 . 손실함수 t(W,b) 정의\n",
        "def loss_func(x,t):\n",
        "    y = np.dot(x,W) + b\n",
        "    return(np.sum((t-y)**2))/(len(x))\n",
        "\n",
        "# Step 4.  수치미분 numerical_derivative및 utility 함수 정의\n",
        "def numerical_derivative(f,x):\n",
        "    delta_x = 1e-5 #0.00001\n",
        "    grad = np.zeros_like(x)\n",
        "\n",
        "    it = np.nditer(x, flags = ['multi_index'], op_flags=['readwrite'])\n",
        "\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        tmp_val = x[idx] \n",
        "        x[idx] = float(tmp_val) + delta_x \n",
        "        fx1 = f(x) \n",
        "\n",
        "        x[idx] = tmp_val - delta_x\n",
        "        fx2 = f(x) \n",
        "        grad[idx] = (fx1-fx2) / (2*delta_x)\n",
        "\n",
        "        x[idx] = tmp_val\n",
        "        it.iternext()\n",
        "    \n",
        "    return grad \n",
        "\n",
        "def loss_val(x,t):\n",
        "    y = np.dot(x,W) + b\n",
        "    return (np.sum((t-y)**2/len(x)))\n",
        "\n",
        "def predict(x):\n",
        "    y = np.dot(x,W) + b\n",
        "    return y\n",
        "\n",
        "# Step 5. 학습률 초기화 및 손실함수가 최소가 될 떄까지 W, b 업데이트\n",
        "learning_rate = 1e-2\n",
        "\n",
        "f = lambda x : loss_func(x_data, t_data)\n",
        "print(loss_val(x_data, t_data),W,b)\n",
        "\n",
        "for step in range(8001):\n",
        "    W -= learning_rate * numerical_derivative(f,W)\n",
        "    b -= learning_rate * numerical_derivative(f,b)\n",
        "\n",
        "    if(step % 400 == 0):\n",
        "        print(step, loss_val(x_data, t_data), W,b)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict(7))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKoc4euhCT1z",
        "outputId": "ee89e015-f6dc-481f-843f-e16e92fafeb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[15.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1. 학습데이터 준비 \n",
        "loaded_data = np.loadtxt('score.csv', delimiter = ',', dtype = np.float32)\n",
        "\n",
        "x_data = loaded_data[:, 0: -1]\n",
        "t_data = loaded_data[:, [-1]]\n",
        "\n",
        "# Step 2 , y = W_1x1+W_2x_2 + W_3x_3+b\n",
        "W = np.random.rand(3,1)\n",
        "b = np.random.rand(1)\n",
        "\n",
        "# Step 3. 손실함수 t(W,b) 정의\n",
        "def loss_func(x,t):\n",
        "    y = np.dot(x,W) + b\n",
        "    return(np.sum((t-y)**2))/(len(x))\n",
        "\n",
        "# Step 4.  수치미분 numerical_derivative및 utility 함수 정의\n",
        "def numerical_derivative(f,x):\n",
        "    delta_x = 1e-4 #0.00001\n",
        "    grad = np.zeros_like(x)\n",
        "\n",
        "    it = np.nditer(x, flags = ['multi_index'], op_flags=['readwrite'])\n",
        "\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        tmp_val = x[idx] \n",
        "        x[idx] = float(tmp_val) + delta_x \n",
        "        fx1 = f(x) \n",
        "\n",
        "        x[idx] = tmp_val - delta_x\n",
        "        fx2 = f(x) \n",
        "        grad[idx] = (fx1-fx2) / (2*delta_x)\n",
        "\n",
        "        x[idx] = tmp_val\n",
        "        it.iternext()\n",
        "    \n",
        "    return grad \n",
        "\n",
        "def loss_val(x,t):\n",
        "    y = np.dot(x,W) + b\n",
        "    return (np.sum((t-y)**2/len(x)))\n",
        "\n",
        "def predict(x):\n",
        "    y = np.dot(x,W) + b\n",
        "    return y\n",
        "\n",
        "# Step 5. 학습률 초기화 및 손실함수가 최소가 될 떄까지 W, b 업데이트\n",
        "learning_rate = 1e-2\n",
        "\n",
        "f = lambda x : loss_func(x_data, t_data)\n",
        "print(loss_val(x_data, t_data),W,b)\n",
        "\n",
        "for step in range(10001):\n",
        "    W -= learning_rate * numerical_derivative(f,W)\n",
        "    b -= learning_rate * numerical_derivative(f,b)\n",
        "\n",
        "    if(step % 400 == 0):\n",
        "        print(step, loss_val(x_data, t_data), W,b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWuwR0WdGXSX",
        "outputId": "2f8ee8b5-2699-40cd-f1db-cf86c3b8b662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2550.159774718336 [[0.74839059]\n",
            " [0.97504824]\n",
            " [0.91884261]] [0.42894313]\n",
            "0 372891052.2185758 [[-79.81270009]\n",
            " [-80.10220449]\n",
            " [-81.75290098]] [390.38406738]\n",
            "400 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "800 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "1200 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "1600 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "2000 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "2400 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "2800 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "3200 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "3600 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "4000 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "4400 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "4800 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "5200 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "5600 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "6000 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "6400 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "6800 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "7200 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "7600 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "8000 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "8400 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "8800 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "9200 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "9600 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n",
            "10000 1.8037322634031636e+29 [[-1.74030942e+12]\n",
            " [-1.74028450e+12]\n",
            " [-1.77373312e+12]] [-2.67865043e+10]\n"
          ]
        }
      ]
    }
  ]
}