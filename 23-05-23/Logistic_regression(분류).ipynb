{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnh10Tv3iLHqDp+01k5b1M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlwodud04/ai-mathematics/blob/main/23-05-23/Logistic_regression(%EB%B6%84%EB%A5%98).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q4lcVtjWLjxN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f8b4487-9b62-4644-b13d-01c2efd4c708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 1) (10, 1)\n",
            "[[0.59678602]] (1, 1) [0.94814914] (1,)\n",
            "intial loss value = 30.914889219992805\n",
            "step =  0 loss value =  13.831915793133117 [[0.18137237]] [0.89560686]\n",
            "step =  5000 loss value =  0.8467242797767777 [[0.89837934]] [-11.52740497]\n",
            "step =  10000 loss value =  0.6215919247150539 [[1.15207088]] [-14.85233754]\n",
            "step =  15000 loss value =  0.509853630071716 [[1.33226931]] [-17.20711276]\n",
            "step =  20000 loss value =  0.4380326922193286 [[1.47710593]] [-19.09713961]\n",
            "step =  25000 loss value =  0.3863364934375962 [[1.60013294]] [-20.70123784]\n",
            "step =  30000 loss value =  0.34667497325005775 [[1.70796557]] [-22.10645731]\n",
            "step =  35000 loss value =  0.31497174223710117 [[1.80441578]] [-23.36286424]\n",
            "step =  40000 loss value =  0.28889123382878684 [[1.89192128]] [-24.50242854]\n",
            "step =  45000 loss value =  0.2669734292125914 [[1.97215653]] [-25.54708561]\n",
            "step =  50000 loss value =  0.24824637390546106 [[2.04633317]] [-26.51269323]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1. 학습데이터 준비\n",
        "x_data = np.array([2,4,6,8,10,12,14,16,18,20]).reshape(10,1)\n",
        "t_data = np.array([0,0,0,0,0,0,1,1,1,1]).reshape(10,1)\n",
        "\n",
        "# Step 2. 임의의 직선 z=Wx+b 정의\n",
        "W = np.random.rand(1,1)\n",
        "b = np.random.rand(1)\n",
        "\n",
        "print(x_data.shape, t_data.shape)\n",
        "print(W,W.shape, b,b.shape)\n",
        "\n",
        "# Step 3. 크로스 엔트로피 정의\n",
        "def sigmoid(z):\n",
        "    return 1 / (1+np.exp(-z))\n",
        "\n",
        "def loss_func(x, t):\n",
        "    delta = 1e-7\n",
        "    z = np.dot(x,W) + b\n",
        "    y = sigmoid(z)\n",
        "\n",
        "    return -np.sum(t*np.log(y+delta) + (1-t)*np.log((1-y)+delta))\n",
        "\n",
        "# Step 4. 수치미분 numerical_derivative및 utility 함수 정의\n",
        "def numerical_derivative(f,x):\n",
        "    delta_x = 1e-4 \n",
        "    grad = np.zeros_like(x)\n",
        "\n",
        "    it = np.nditer(x, flags = ['multi_index'], op_flags=['readwrite'])\n",
        "\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        tmp_val = x[idx] \n",
        "        x[idx] = float(tmp_val) + delta_x \n",
        "        fx1 = f(x) \n",
        "\n",
        "        x[idx] = tmp_val - delta_x\n",
        "        fx2 = f(x) \n",
        "        grad[idx] = (fx1-fx2) / (2*delta_x)\n",
        "\n",
        "        x[idx] = tmp_val\n",
        "        it.iternext()\n",
        "    \n",
        "    return grad\n",
        "\n",
        "def loss_val(x,t):\n",
        "    delta = 1e-7\n",
        "    z = np.dot(x,W)+b\n",
        "    y = sigmoid(z)\n",
        "\n",
        "    return -np.sum(t*np.log(y+delta) + (1-t)*np.log((1-y)+delta))\n",
        "\n",
        "def predict(test_data):\n",
        "    z = np.dot(test_data,W)+b\n",
        "    y = sigmoid(z)\n",
        "\n",
        "    if y >= 0.5:\n",
        "        result = 1\n",
        "    else : \n",
        "        result = 0\n",
        "    \n",
        "    return y,result\n",
        "\n",
        "# Step 5. 학습을 초기화 및 손실함수가 최소가 될 때까지 W, b 업데이트\n",
        "\n",
        "learning_rate = 1e-2\n",
        "f = lambda x : loss_func(x_data, t_data)\n",
        "print(\"intial loss value =\", loss_val(x_data, t_data))\n",
        "\n",
        "for step in range(50001):\n",
        "    W -= learning_rate * numerical_derivative(f,W)\n",
        "    b -= learning_rate * numerical_derivative(f,b)\n",
        "\n",
        "    if(step % 5000 == 0):\n",
        "        print(\"step = \", step,\"loss value = \", loss_val(x_data, t_data), W,b)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 공부시간 3시간과 17시간에 대한 미래 값 예측\n",
        "\n",
        "test_data = np.array([3,0])\n",
        "(real_val_1, logical_val_1) = predict(test_data)\n",
        "print(real_val_1, logical_val_1)\n",
        "\n",
        "test_data = np.array([17,0])\n",
        "(real_val_2, logical_val_2) = predict(test_data)\n",
        "print(real_val_2, logical_val_2)"
      ],
      "metadata": {
        "id": "sARy0ldFdTZk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}